# Evaluating Information Retrieval Performance on Human-written and AI-generated News Articles

#### _AIR - WS2025 - Group 07_

|                       |                    |                              |
| :-------------------: | :----------------: | :--------------------------: |
|     **Erik Koppenhagen**    | **Simon Fessl** |     **Jasmin Bauer**     |
| Dataset, Preprocessing |      Methods/Models     | Evaluation |

In the recent years, the use of large language models (LLMs) has led to a rapid increase of AI-
generated content on the internet. Specifically in journalism the use of AI also increased.
The texts of news articles are revised, rewritten or sometimes even completely generated by AI. Thus, search engines and information retrieval systems are faced with the challenge of jointly indexing and evaluating both human-written and fully AI-generated texts according to their relevance. This raises the important question, whether traditional and neural information retrieval models process both types of text equally well, or whether systematic differences occur regarding the retrieval quality. Exactly in this growing relevance of mixed corpora of human- and AI-generated news articles lies the motivation for this project. Because the information systems have to ensure that relevant content is reliably found and ranked correctly according to its relevance, regardless of its origin. But AI-generated text exhibits some different linguistic and semantic properties in comparison to human-written text and this could affect the effectiveness of existing information retrieval models and lead to a bias in the search results, namely that AI-generated news articles are preferentially retrieved. Because of this, this project pursues to answer the following research question: ”How do completely AI-generated news articles influence the retrieval effectiveness of traditional and neural information retrieval models?”

## Code structure 

#### Data scraping and prepocessing
- get_tageschau_links.ipynb -> creates a list of links to all Tageschau articles of the past 2 years and saves the articles as JSON.
- tageschau_paster.ipynb -> parses the JSON files and extracts the important parts of the articles.
#### Generating LLM Text
- bullets.py -> uses LLM to summarize the article text in a list of bullet points
- bullets_to_text.py -> uses LLM to write a news article based on the bullet points
#### Information retrieval and evaluation
- information_retrival_pipeline.py -> retrieves top k articles with BM25 and bi-encoder
- run information_retrteval_pipeline.ipynb -> evaluates results and plots data.

## Dataset Descripiton
We scraped news articles from the "Tagesschau" published on their website over the last 2 years, from 11/07/2023 to 11/07/2025 and got 7.191 human-written news articles. We cleaned this dataset by removing the HTML tags, and for computational reasons, we filtered out all articles with more than 1200 tokens. This left us with 4.938 articles. Next, these articles were summarized by an LLM into bullet points and then, AI-generated news articles were generated from these bullet points by another LLM.

## Reproduction of Results

In the "requirements.txt" file you can see a list of all dependencies needed to run the code.
To reproduce our results, you first need to scrape the "Tagesschau" website using the "get_tageschau_links.ipynb" file to get the data. Then you can parse the resulting data into a pandas dataframe using the "tageschau_paster.ipynb" file.  
After that, you can generate LLM summaries with the "bullets.py" file. Here you have to provide an API key for Hugging Face and also need to have a computer capable of running an LLM. Those summaries can then be converted into text again by the "bullets_to_text.py" file. For that, you also need the API key and a computer with the necessary power. 
For the mixed human-written and Ai-generated text corpus, you can then use the "information_retrteval_pipeline.ipynb" file to run the BM25 and bi- and cross-encoder pipeline to retrieve the articles from the news article corpus by querying with the tags of an article.
