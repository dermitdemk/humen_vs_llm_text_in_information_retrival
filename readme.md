# Evaluating Information Retrieval Performance on Human-written and AI-generated News Articles

#### _AIR - WS2025 - Group 07_

|                       |                    |                              |
| :-------------------: | :----------------: | :--------------------------: |
|     **Erik Koppenhagen**    | **Simon Fessl** |     **Jasmin Bauer**     |
| Dataset, Preprocessing |      Methods/Models     | Evaluation |

In the recent years, the use of large language models (LLMs) has led to a rapid increase of AI-
generated content on the internet. Specifically in journalism the use of AI also increased.
The texts of news articles are revised, rewritten or sometimes even completely generated by AI. Thus, search engines and information retrieval systems are faced with the challenge of jointly indexing and evaluating both human-written and fully AI-generated texts according to their relevance. This raises the important question, whether traditional and neural information retrieval models process both types of text equally well, or whether systematic differences occur regarding the retrieval quality. Exactly in this growing relevance of mixed corpora of human- and AI-generated news articles lies the motivation for this project. Because the information systems have to ensure that relevant content is reliably found and ranked correctly according to its relevance, regardless of its origin. But AI-generated text exhibits some different linguistic and semantic properties in comparison to human-written text and this could affect the effectiveness of existing information retrieval models and lead to a bias in the search results, namely that AI-generated news articles are preferentially retrieved. Because of this, this project pursues to answer the following research question: ”How do completely AI-generated news articles influence the retrieval effectiveness of traditional and neural information retrieval models?”

## Project structure / Experimental Setup 

#### Data scraping and Prepocessing
- get_tagesschau_links.ipynb -> Creates a list of links to all ["Tagesschau"](https://www.tagesschau.de) articles of the past 2 years and saves the articles as JSON.
- tagesschau_paster.ipynb -> Parses the JSON files and extracts the important parts of the articles.
- clean_articles.py -> extracts subtitle from text, cleans html chunk, produces clean useable csv from scraped json data
#### Generating LLM Text
- bullets.py -> Uses the LLM to summarize the text of the news articles in a list of bullet points.
- bullets_to_text.py -> Uses the LLM to write news articles based on the bullet points.
#### Information Retrieval and Evaluation
- evaluate_per_tag.py -> Computes evaluation metrics and plots data per tag
- information_retrieval_pipeline.py -> Retrieves top 5 articles with BM25 and bi- and cross-encoder pipeline.
- run_information_retrieval_pipeline.ipynb -> Evaluates results and plots data.

## Reproduction of Results

In the "requirements.txt" file you can see a list of all dependencies needed to run the code.  
To reproduce our results, you first need to scrape the ["Tagesschau"](https://www.tagesschau.de) website using the "get_tagesschau_links.ipynb" file to get the data. Then you can parse the resulting data into a pandas dataframe using the "tagesschau_paster.ipynb" file. For subtitle extraction and removal of html tags and redundancies, the "clean_articles.py" file should be used to receive a clean corpus CSV of the articles.  
After that, you can generate LLM summaries with the "bullets.py" file. Here you have to provide an API key for Hugging Face and also need to have a computer capable of running an LLM. Those summaries can then be converted into text again by the "bullets_to_text.py" file. For that, you also need the API key and a computer with the necessary power.    
For the mixed human-written and AI-generated text corpus, you can then use the "information_retrieval_pipeline.ipynb" file to run the BM25 and bi- and cross-encoder pipeline to retrieve the articles from the news article corpus by querying with the tags of an article. Finally, you can run the "run_information_retrieval_pipeline.ipynb" file see the evaluation and the visualizations of the results. For the evaluation per tag and retrieval metrics, the "evaluate_per_tag.py" file can be run. Plots can then be found in the created /retrieval_eval_out folder, metrics and information about the pipeline in stdout.

## Dataset Description
We scraped news articles from the ["Tagesschau"](https://www.tagesschau.de) published on their website over the last 2 years, from 11/07/2023 to 11/07/2025 and got 7190 human-written news articles. We cleaned this dataset by removing the HTML tags, and for computational reasons, we filtered out all articles with more than 1200 tokens. This left us with 4091 articles. Next, these articles were summarized by an LLM into bullet points and then, AI-generated news articles were generated from these bullet points by another LLM.
